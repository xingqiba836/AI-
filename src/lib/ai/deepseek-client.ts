import OpenAI from 'openai';
import { AI_CONFIG, validateAIConfig, AI_ERROR_MESSAGES } from './ai-config';

// DeepSeek å®¢æˆ·ç«¯ç±»
export class DeepSeekClient {
  private client: OpenAI;
  
  constructor() {
    // éªŒè¯é…ç½®
    const validation = validateAIConfig();
    if (!validation.valid) {
      throw new Error(validation.error);
    }
    
    // åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ï¼ˆDeepSeek API å…¼å®¹ OpenAI æ ¼å¼ï¼‰
    this.client = new OpenAI({
      apiKey: AI_CONFIG.apiKey,
      baseURL: AI_CONFIG.baseURL,
      timeout: AI_CONFIG.timeout,
      maxRetries: AI_CONFIG.retry.maxRetries,
    });
    
    console.log('âœ… DeepSeek å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ');
  }
  
  /**
   * å‘é€èŠå¤©è¯·æ±‚
   * @param messages æ¶ˆæ¯åˆ—è¡¨
   * @param options å¯é€‰å‚æ•°
   */
  async chat(
    messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }>,
    options?: {
      temperature?: number;
      maxTokens?: number;
      stream?: boolean;
    }
  ): Promise<string> {
    try {
      console.log('ğŸ“¤ å‘é€è¯·æ±‚åˆ° DeepSeek API...');
      console.log('æ¶ˆæ¯æ•°é‡:', messages.length);
      
      const response = await this.client.chat.completions.create({
        model: AI_CONFIG.model,
        messages: messages as any,
        temperature: options?.temperature ?? AI_CONFIG.generation.temperature,
        max_tokens: options?.maxTokens ?? AI_CONFIG.generation.maxTokens,
        top_p: AI_CONFIG.generation.topP,
        presence_penalty: AI_CONFIG.generation.presencePenalty,
        frequency_penalty: AI_CONFIG.generation.frequencyPenalty,
        stream: options?.stream ?? false,
      });
      
      const content = response.choices[0]?.message?.content;
      
      if (!content) {
        throw new Error('AI è¿”å›å†…å®¹ä¸ºç©º');
      }
      
      console.log('âœ… æ”¶åˆ° AI å“åº”');
      console.log('Token ä½¿ç”¨:', response.usage);
      
      return content;
    } catch (error: any) {
      console.error('âŒ DeepSeek API è°ƒç”¨å¤±è´¥:', error);
      throw this.handleError(error);
    }
  }
  
  /**
   * æµå¼èŠå¤©è¯·æ±‚
   * @param messages æ¶ˆæ¯åˆ—è¡¨
   * @param onChunk æ¥æ”¶åˆ°æ¯ä¸ª chunk çš„å›è°ƒ
   */
  async chatStream(
    messages: Array<{ role: 'system' | 'user' | 'assistant'; content: string }>,
    onChunk: (chunk: string) => void
  ): Promise<string> {
    try {
      console.log('ğŸ“¤ å‘é€æµå¼è¯·æ±‚åˆ° DeepSeek API...');
      
      const stream = await this.client.chat.completions.create({
        model: AI_CONFIG.model,
        messages: messages as any,
        temperature: AI_CONFIG.generation.temperature,
        max_tokens: AI_CONFIG.generation.maxTokens,
        top_p: AI_CONFIG.generation.topP,
        stream: true,
      });
      
      let fullContent = '';
      
      for await (const chunk of stream) {
        const content = chunk.choices[0]?.delta?.content || '';
        if (content) {
          fullContent += content;
          onChunk(content);
        }
      }
      
      console.log('âœ… æµå¼å“åº”å®Œæˆ');
      return fullContent;
    } catch (error: any) {
      console.error('âŒ DeepSeek æµå¼ API è°ƒç”¨å¤±è´¥:', error);
      throw this.handleError(error);
    }
  }
  
  /**
   * å¤„ç† API é”™è¯¯
   */
  private handleError(error: any): Error {
    // OpenAI SDK é”™è¯¯
    if (error.response) {
      const status = error.response.status;
      const errorCode = error.response.data?.error?.code;
      
      console.error('API é”™è¯¯çŠ¶æ€:', status);
      console.error('é”™è¯¯ä»£ç :', errorCode);
      
      // æ ¹æ®é”™è¯¯ç è¿”å›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
      if (status === 401) {
        return new Error(AI_ERROR_MESSAGES.invalid_api_key);
      } else if (status === 429) {
        return new Error(AI_ERROR_MESSAGES.rate_limit_exceeded);
      } else if (status === 402 || errorCode === 'insufficient_quota') {
        return new Error(AI_ERROR_MESSAGES.insufficient_quota);
      } else if (status === 404) {
        return new Error(AI_ERROR_MESSAGES.model_not_found);
      }
    }
    
    // ç½‘ç»œé”™è¯¯
    if (error.code === 'ECONNABORTED' || error.message?.includes('timeout')) {
      return new Error(AI_ERROR_MESSAGES.timeout);
    }
    
    if (error.code === 'ENOTFOUND' || error.message?.includes('network')) {
      return new Error(AI_ERROR_MESSAGES.network_error);
    }
    
    // å…¶ä»–é”™è¯¯
    return new Error(
      error.message || AI_ERROR_MESSAGES.unknown_error
    );
  }
  
  /**
   * æµ‹è¯• API è¿æ¥
   */
  async testConnection(): Promise<{ success: boolean; error?: string }> {
    try {
      console.log('ğŸ” æµ‹è¯• DeepSeek API è¿æ¥...');
      
      const response = await this.chat([
        { role: 'user', content: 'ä½ å¥½ï¼Œè¯·ç®€çŸ­å›å¤ã€‚' },
      ]);
      
      if (response) {
        console.log('âœ… API è¿æ¥æµ‹è¯•æˆåŠŸ');
        return { success: true };
      }
      
      return { success: false, error: 'æœªæ”¶åˆ°å“åº”' };
    } catch (error: any) {
      console.error('âŒ API è¿æ¥æµ‹è¯•å¤±è´¥:', error);
      return {
        success: false,
        error: error.message || 'è¿æ¥å¤±è´¥',
      };
    }
  }
}

// åˆ›å»ºå•ä¾‹å®ä¾‹
let deepSeekClient: DeepSeekClient | null = null;

/**
 * è·å– DeepSeek å®¢æˆ·ç«¯å®ä¾‹
 */
export function getDeepSeekClient(): DeepSeekClient {
  if (!deepSeekClient) {
    deepSeekClient = new DeepSeekClient();
  }
  return deepSeekClient;
}

